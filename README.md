# DATA-PIPELINE-DEVELOPMENT

*COMPANY*: CODTECH IT SOLUTIONS

*NAME*: G. HARINI PRIYA

*INTERN ID*: CT04DN652

*DOMAIN*: DATA SCIENCE

*DURATION*: 4 WEEKS

*MENTOR*: NEELA SANTOSH

## Project Overview
This project demonstrates an automated ETL (Extract, Transform, Load) pipeline implemented in Python, designed to process raw student data for further analysis or machine learning tasks. The *ETL process* is essential in data engineering to prepare and clean data, handling inconsistencies and encoding categorical variables effectively. This pipeline reads raw data from a CSV file, transforms it using popular Python libraries like `pandas` and `scikit-learn`, and saves the processed data into a new CSV file, ready for analysis.

## Project Structure
The project contains the following key files:
- `dataPipeline.py`: The main Python script that automates the ETL process, including extraction, transformation, and loading.
- `data.csv`: The raw dataset containing student information with some missing and categorical data.
- `processed_data.csv`: The output file generated by the pipeline, containing cleaned and transformed data.
- `README.md`: This documentation file explaining the project details and instructions.

## Technologies and Libraries Used
The pipeline leverages the following libraries:
- `pandas`:  For efficient data loading, manipulation, and storage.
- `scikit-learn`:  For preprocessing tasks including missing value imputation, feature scaling, and encoding categorical variables.
- `Python 3.x`:  The programming language used for scripting.

## Installation and Setup
To run this project on your local machine, you must have Python 3.x installed. Additionally, the required Python packages need to be installed using pip:
*pip install pandas scikit-learn*

## Running the ETL Pipeline
1.	Open the project folder in VS Code.
2.	Open the dataPipeline.py file.
3.	Run the script by clicking the green "Run" button or by opening the terminal and running:
`python dataPipeline.py`

## The script will:
- Extract the raw data from data.csv.
- Transform the data by filling missing numerical values with the mean and missing categorical values with the most frequent category.
-	Standardize numerical features such as age and roll number.
-	One-hot encode categorical variables like the department.
-	Load the processed data into processed_data.csv.

## Data Description and Sample Input
The raw input dataset, data.csv, contains student records with columns such as name, age, dept (department), rollno (roll number), and marks (student scores). Some values are missing, particularly in the dept column.

`You can find the sample data in data.csv and the processed data is output which is in processed_data.csv`

## Data Transformation Details
During the transformation step:
-	`Numerical Imputation`: Missing values in numerical columns are filled using the mean of the available values.
-	`Categorical Imputation`: Missing categories are filled with the mode (most frequent category).
-	`Feature Scaling`: Numerical features are scaled using standardization to ensure features are on a comparable scale.
-	`One-Hot Encoding`: Categorical variables are converted into binary indicator variables, allowing machine learning algorithms to utilize them effectively.
These preprocessing steps improve data quality and model readiness.

## Output
The final output, processed_data.csv, contains the cleaned and transformed dataset. It is structured so that all features are numeric, missing values are handled, and categorical data is encoded. This file is ready for further analysis or use in machine learning models.

## Use Case and Applications
This ETL pipeline is suitable for preprocessing educational data or similar datasets where cleaning and encoding are necessary. Automating such pipelines saves time and minimizes manual errors, which is crucial when dealing with large datasets in industries such as education analytics, business intelligence, or any domain requiring reliable data preparation.

## Conclusion
This project highlights the importance of automating the ETL process using Python and standard libraries. It provides a reusable framework for preparing raw datasets into a format suitable for analysis and modeling. Using VS Code as the development environment makes it easy to edit, run, and maintain the pipeline.

